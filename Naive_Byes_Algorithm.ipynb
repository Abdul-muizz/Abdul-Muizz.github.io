{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Naive Byes Algorithm.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdul-muizz/Abdul-Muizz.github.io/blob/main/Naive_Byes_Algorithm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3tlQOxZ2pLC",
        "outputId": "b35f15b5-32a1-4118-e569-68d81ac1694f"
      },
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import sklearn as sk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras import utils as np_utils\n",
        "from keras.layers.merge import concatenate\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "\n",
        "import keras\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import imblearn\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, cohen_kappa_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/CICIDS2017/\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/CICIDS2017\n",
            "/content/drive/MyDrive/CICIDS2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua95X8jZ2tXo"
      },
      "source": [
        "#db1 = pd.read_csv('Monday-WorkingHours.csv', sep=',')\n",
        "db2 = pd.read_csv('Tuesday-WorkingHours.csv', sep=',')\n",
        "#db3 = pd.read_csv('Wednesday-workingHours.csv', sep=',')\n",
        "#db4 = pd.read_csv('Thursday-WorkingHours.csv', sep=',')\n",
        "#db5 = pd.read_csv('Thursday-WorkingHours-Afternoon-Infilteration.csv', sep=',')\n",
        "#db6 = pd.read_csv('Friday-WorkingHours-Morning.csv', sep=',')\n",
        "#db7 = pd.read_csv('Friday-WorkingHours-Afternoon-PortScan.csv', sep=',')\n",
        "#db8 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.csv', sep=',')\n",
        "#full_db = pd.concat([db1, db2, db3, db4, db5, db6, db7, db8])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9Cz7DXQaUdT"
      },
      "source": [
        "#import gc\n",
        "#del db1, db2, db3, db4, db5, db6, db7, db8\n",
        "#gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-lTJjxG24-X",
        "outputId": "6695e713-f7b3-4f24-84ae-527b5fa674a4"
      },
      "source": [
        "# full_db = full_db.drop_duplicates(keep=\"first\")\n",
        "db2 =db2[db2['Label'] != 'Heartbleed']\n",
        "db2 = db2[db2['Label'] != 'Infiltration']\n",
        "db2 = db2[db2['Label'] != 'Web Attack � Sql Injection']\n",
        "db2 = db2[db2['Label'] != 'Web Attack � XSS']\n",
        "db2 = db2[db2['Label'] != 'Web Attack � Brute Force']\n",
        "web1 = db2[db2['Label'] == 'Web Attack � Sql Injection']\n",
        "web2 = db2[db2['Label'] == 'Web Attack � XSS']\n",
        "web3 = db2[db2['Label'] == 'Web Attack � Brute Force']\n",
        "web = pd.concat([web1, web2, web3])\n",
        "web[' Label'] = 'Web Attack'\n",
        "# full_db1 = pd.concat([full_db1, web])\n",
        "print(db2.shape, db2['Label'].value_counts())\n",
        "print(db2.shape, db2['Label'].value_counts())\n",
        "print(web.shape, web['Label'].value_counts())\n",
        "# print(100*full_db[' Label'].value_counts(normalize=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(322003, 84) BENIGN                     315031\n",
            "FTP-Patator                  3973\n",
            "SSH-Patator                  2980\n",
            "FTP-Patator - Attempted        11\n",
            "SSH-Patator - Attempted         8\n",
            "Name: Label, dtype: int64\n",
            "(322003, 84) BENIGN                     315031\n",
            "FTP-Patator                  3973\n",
            "SSH-Patator                  2980\n",
            "FTP-Patator - Attempted        11\n",
            "SSH-Patator - Attempted         8\n",
            "Name: Label, dtype: int64\n",
            "(0, 85) Series([], Name: Label, dtype: int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdscPjcgZkw2"
      },
      "source": [
        "#import gc\n",
        "#del full_db, web\n",
        "#gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8TZWCVW4K2t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bec828ea-5a44-48a0-d6f8-9604809a21b1"
      },
      "source": [
        "min_max_scaler = MinMaxScaler() \n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# db = full_db\n",
        "db = db2[~db2.isin([np.inf]).any(1)]\n",
        "df = db\n",
        "del df['Flow ID'], df['Src IP'], df['Dst IP'], df['Timestamp']\n",
        "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
        "\n",
        "trainX1, testX, trainY1, testY = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "trainY1=le.fit_transform(trainY1.to_numpy()).astype('int')\n",
        "testY=le.fit_transform(testY.to_numpy()).astype('int')\n",
        "\n",
        "trainX1 = min_max_scaler.fit_transform(trainX1)\n",
        "testX = min_max_scaler.fit_transform(testX)\n",
        "\n",
        "print(trainX1.shape, trainY1.shape, testX.shape, testY.shape)\n",
        "trainX, trainY = trainX1, trainY1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(257584, 79) (257584,) (64396, 79) (64396,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yp3wpzE7J9n"
      },
      "source": [
        "#import gc\n",
        "#del trainX1, trainY1, full_db1, db, X, y\n",
        "#gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqCyYHtzzP5n"
      },
      "source": [
        "#from collections import Counter\n",
        "#from imblearn.under_sampling import RandomUnderSampler\n",
        "#d = {0:1000, 1:500, 2:1000, 3:500, 4:1000, 5:500, 6:500, 7:500, 8:5, 9:10, 10:1000, 11:500, 12:500, 13:5, 14: 100}\n",
        "#undersample = RandomUnderSampler(sampling_strategy=d)\n",
        "#counter = Counter(trainY)\n",
        "#print(counter)\n",
        "#X, y = undersample.fit_resample(trainX, trainY)\n",
        "#counter = Counter(y)\n",
        "#print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KOhg5XT3cgK"
      },
      "source": [
        "trainX = X\n",
        "trainY = y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ9whYeXqrrl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbe3d528-25cf-4d1f-d0ad-a71caf4c156e"
      },
      "source": [
        "model = GaussianNB()\n",
        "print(\"Naive Bayes\")\n",
        "\n",
        "model = model.fit(trainX, trainY)\n",
        "\n",
        "predY = model.predict(testX)\n",
        "# cm = confusion_matrix(testY, predY)\n",
        "\n",
        "correct_pred = (testY == predY)\n",
        "acc = correct_pred.sum() / len(correct_pred)\n",
        "# print(cm)\n",
        "print(\"Acc: \", acc*100)\n",
        "print(classification_report(testY, predY))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naive Bayes\n",
            "Acc:  97.91136095409652\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99     63051\n",
            "           1       0.00      0.00      0.00       761\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.00      0.00      0.00       579\n",
            "           4       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.98     64396\n",
            "   macro avg       0.20      0.20      0.20     64396\n",
            "weighted avg       0.96      0.98      0.97     64396\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1-SywcJ68P6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}