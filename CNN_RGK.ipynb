{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-RGK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abdul-muizz/Abdul-Muizz.github.io/blob/main/CNN_RGK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3tlQOxZ2pLC",
        "outputId": "e7ba7c74-04cb-4f6c-98d3-bb99d44fa564"
      },
      "source": [
        "import datetime\n",
        "import pandas as pd\n",
        "from sklearn import tree\n",
        "import time\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import svm\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "import sklearn as sk\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from numpy import loadtxt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Model, Sequential\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
        "from keras import utils as np_utils\n",
        "from keras.layers.merge import concatenate\n",
        "#from keras.layers.normalization import BatchNormalization\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Conv1D, MaxPooling1D, Flatten, Reshape\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.layers.wrappers import TimeDistributed\n",
        "\n",
        "import keras\n",
        "from sklearn import preprocessing\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "import imblearn\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch\n",
        "from sklearn.metrics import classification_report, cohen_kappa_score\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/CICIDS2017/\n",
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/CICIDS2017\n",
            "/content/drive/MyDrive/CICIDS2017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua95X8jZ2tXo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f45bfa52-95d3-4df1-cd71-3844ed8ab166"
      },
      "source": [
        "#db1 = pd.read_csv('Monday-WorkingHours.csv', sep=',')\n",
        "db2 = pd.read_csv('Tuesday-WorkingHours.csv', sep=',')\n",
        "#db3 = pd.read_csv('Wednesday-workingHours.pcap_ISCX.csv', sep=',')\n",
        "#db4 = pd.read_csv('Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv', sep=',')\n",
        "#db4 = pd.read_csv('Thursday-WorkingHours.csv', sep=',')\n",
        "#db5 = pd.read_csv('Friday-WorkingHours.csv', sep=',')\n",
        "#db7 = pd.read_csv('Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv', sep=',')\n",
        "#db8 = pd.read_csv('Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv', sep=',')\n",
        "#full_db = pd.concat([db1, db2, db3, db4, db5, db6, db7, db8])\n",
        "\n",
        "#print(full_db.shape, full_db[' Label'].value_counts())\n",
        "#print(100*full_db[' Label'].value_counts(normalize=True))\n",
        "print(db2.shape, db2['Label'].value_counts())\n",
        "print(100*db2['Label'].value_counts(normalize=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(322003, 84) BENIGN                     315031\n",
            "FTP-Patator                  3973\n",
            "SSH-Patator                  2980\n",
            "FTP-Patator - Attempted        11\n",
            "SSH-Patator - Attempted         8\n",
            "Name: Label, dtype: int64\n",
            "BENIGN                     97.834803\n",
            "FTP-Patator                 1.233839\n",
            "SSH-Patator                 0.925457\n",
            "FTP-Patator - Attempted     0.003416\n",
            "SSH-Patator - Attempted     0.002484\n",
            "Name: Label, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min_max_scaler = MinMaxScaler() \n",
        "le = preprocessing.LabelEncoder()\n",
        "\n",
        "# db = full_db (for now we are usind db2)\n",
        "db = db2[~db2.isin([np.inf]).any(1)]\n",
        "df = db\n",
        "#df = pd.DataFrame(db)\n",
        "#df.drop(['Flow ID'], axis = 1)\n",
        "#df.drop(['Src IP'], axis = 1)\n",
        "#df.drop(['Dst IP'], axis = 1)\n",
        "#df.drop(['Timestamp'], axis = 1)\n",
        "del df['Flow ID'], df['Src IP'], df['Dst IP'], df['Timestamp']\n",
        "#print(df.dtypes)\n",
        "X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
        "\n",
        "trainX1, testX, trainY1, testY = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "trainY1=le.fit_transform(trainY1.to_numpy()).astype('int')\n",
        "testY=le.fit_transform(testY.to_numpy()).astype('int')\n",
        "\n",
        "trainX1 = min_max_scaler.fit_transform(trainX1)\n",
        "testX = min_max_scaler.fit_transform(testX)\n",
        "\n",
        "print(trainX1.shape, trainY1.shape, testX.shape, testY.shape)\n",
        "trainX, trainY = trainX1, trainY1\n",
        "\n",
        "X1 = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
        "X2 = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
        "Y1 = np.reshape(trainY, (trainY.shape[0], 1))\n",
        "Y1 = tf.keras.utils.to_categorical(Y1)\n",
        "Y2 = np.reshape(testY, (testY.shape[0], 1))\n",
        "Y2 = tf.keras.utils.to_categorical(Y2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xr1f1IfUp7lF",
        "outputId": "62300c44-a9b4-485c-90f0-6998e1547e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(257584, 79) (257584,) (64396, 79) (64396,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y-lTJjxG24-X"
      },
      "source": [
        "#import gc\n",
        "#del db1, db2, db3, db4, db5, db6, db7, db8, db, full_db, X, y, trainX1, trainY1, trainX, trainY, testX, testY\n",
        "#gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqCyYHtzzP5n"
      },
      "source": [
        "# from collections import Counter\n",
        "# from imblearn.under_sampling import RandomUnderSampler\n",
        "# d = {0:1000, 1:500, 2:1000, 3:500, 4:1000, 5:500, 6:500, 7:500, 8:5, 9:10, 10:1000, 11:500, 12:500, 13:5, 14: 100}\n",
        "# undersample = RandomUnderSampler(sampling_strategy=d)\n",
        "# counter = Counter(trainY)\n",
        "# print(counter)\n",
        "# X, y = undersample.fit_resample(trainX, trainY)\n",
        "# counter = Counter(y)\n",
        "# print(counter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KOhg5XT3cgK"
      },
      "source": [
        "trainX = X\n",
        "trainY = y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e--DGVrH4LGv"
      },
      "source": [
        "def keras_cnn_model(X, Y):\n",
        "\n",
        "    X_train_model = X\n",
        "    Y_train_model = Y\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(filters=50, kernel_size=3, activation='softmax', input_shape=(X_train_model.shape[1], X_train_model.shape[2]), kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    # model.add(Conv1D(filters=50, kernel_size=3, activation='softmax'))\n",
        "    # model.add(Dropout(0.5))\n",
        "    # model.add(MaxPooling1D(pool_size=2))\n",
        "    model.add(Flatten())\n",
        "    # model.add(Dense(100, activation='softmax'))\n",
        "    model.add(Dense(Y_train_model.shape[1], activation='softmax', kernel_initializer='random_uniform', bias_initializer='zeros'))\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model.summary()\n",
        "    return model\n",
        "\n",
        "def keras_cnn_fit(model, X, Y, X1, Y1, epochs, batch_size):\n",
        "    history = model.fit(X, Y, epochs=epochs, batch_size=batch_size, verbose = 1, validation_data=(X1, Y1))\n",
        "    return history\n",
        "\n",
        "def plot_keras_cnn(history):\n",
        "\n",
        "    plt.plot(history.history['acc'])\n",
        "    plt.plot(history.history['val_acc'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()\n",
        "    # summarize history for loss\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('model loss')\n",
        "    plt.ylabel('loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['train', 'test'], loc='upper left')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9hytJ-IU0tC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95f78441-fff4-42a6-a152-c53950b5d90b"
      },
      "source": [
        "modelCnn = keras_cnn_model(X1, Y1)\n",
        "histCnn = keras_cnn_fit(modelCnn, X1, Y1, X2, Y2, 10, 256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d (Conv1D)             (None, 77, 50)            200       \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 3850)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5)                 19255     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 19,455\n",
            "Trainable params: 19,455\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "1007/1007 [==============================] - 23s 22ms/step - loss: 0.1366 - accuracy: 0.9772 - val_loss: 0.1069 - val_accuracy: 0.9791\n",
            "Epoch 2/10\n",
            "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0793 - accuracy: 0.9782 - val_loss: 0.0590 - val_accuracy: 0.9791\n",
            "Epoch 3/10\n",
            "1007/1007 [==============================] - 20s 20ms/step - loss: 0.0568 - accuracy: 0.9780 - val_loss: 0.0475 - val_accuracy: 0.9879\n",
            "Epoch 4/10\n",
            "1007/1007 [==============================] - 24s 24ms/step - loss: 0.0461 - accuracy: 0.9797 - val_loss: 0.0399 - val_accuracy: 0.9891\n",
            "Epoch 5/10\n",
            "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0341 - accuracy: 0.9887 - val_loss: 0.0272 - val_accuracy: 0.9953\n",
            "Epoch 6/10\n",
            "1007/1007 [==============================] - 21s 21ms/step - loss: 0.0210 - accuracy: 0.9949 - val_loss: 0.0314 - val_accuracy: 0.9915\n",
            "Epoch 7/10\n",
            "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0112 - accuracy: 0.9994 - val_loss: 0.0255 - val_accuracy: 0.9918\n",
            "Epoch 8/10\n",
            "1007/1007 [==============================] - 18s 18ms/step - loss: 0.0065 - accuracy: 0.9997 - val_loss: 0.0257 - val_accuracy: 0.9915\n",
            "Epoch 9/10\n",
            "1007/1007 [==============================] - 19s 19ms/step - loss: 0.0044 - accuracy: 0.9997 - val_loss: 0.0288 - val_accuracy: 0.9888\n",
            "Epoch 10/10\n",
            "1007/1007 [==============================] - 20s 19ms/step - loss: 0.0034 - accuracy: 0.9997 - val_loss: 0.0249 - val_accuracy: 0.9913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-aB8ZFYcNQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7f18e2a-0509-4bf5-b524-178bfd01243e"
      },
      "source": [
        "predictions = modelCnn.predict(X2)\n",
        "\n",
        "predictions2 = np.argmax(predictions, axis=1)\n",
        "Y22 = np.argmax(Y2, axis=1)\n",
        "Y22.shape, predictions2.shape\n",
        "\n",
        "cm = confusion_matrix(Y22, predictions2)\n",
        "correct_pred = (Y22 == predictions2)\n",
        "acc = correct_pred.sum() / len(correct_pred)\n",
        "print(cm)\n",
        "print(\"Acc: \", acc*100)\n",
        "print(classification_report(Y22, predictions2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[62503   461     0    87     0]\n",
            " [    0   761     0     0     0]\n",
            " [    4     0     0     0     0]\n",
            " [    2     3     0   574     0]\n",
            " [    1     0     0     0     0]]\n",
            "Acc:  99.13348655195975\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.99      1.00     63051\n",
            "           1       0.62      1.00      0.77       761\n",
            "           2       0.00      0.00      0.00         4\n",
            "           3       0.87      0.99      0.93       579\n",
            "           4       0.00      0.00      0.00         1\n",
            "\n",
            "    accuracy                           0.99     64396\n",
            "   macro avg       0.50      0.60      0.54     64396\n",
            "weighted avg       0.99      0.99      0.99     64396\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1-SywcJ68P6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}